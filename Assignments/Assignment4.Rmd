---
title: "Assignment4"
author: "Yufei Yin"
output: pdf_document
---

# Exercise 1 (3 points): 

Chapter 3, Exercise 9 from Bayesian Data Analysis, 3rd Edition (Gelman et al.), which is linked to from the course Canvas page.

## Chapter 3, Exercise 9

Conjugate normal model: suppose y is an independent and identically distributed sample of size n from the distribution $N(\mu, \sigma^2)$, where the prior distribution for $(\mu, \sigma^2)$ is $N-Inv-\chi^2(\mu,\sigma^2|\mu_0,\sigma_0^2/\kappa_0;\upsilon_0,\sigma_0^2)$; that is, $\sigma^2 \sim Inv-\chi^2(\upsilon_0,\sigma_0^2)$ and $\mu|\sigma \sim N(\mu_0,\sigma^2/\kappa_0)$. The posterior distribution, $p(\mu,\sigma^2|y)$, is also normal-inverse-$\chi^2$; derive explicitly its parameters in terms of the prior parameters and the sufficient statistics of the data.

\bigskip

$\mu|\sigma \sim N(\mu_0,\sigma^2/\kappa_0)$

$\sigma^2 \sim Inv-\chi^2(\upsilon_0,\sigma_0^2)$

# joint prior density

$N-Inv-\chi^2(\mu,\sigma^2|\mu_0,\sigma_0^2/\kappa_0;\upsilon_0,\sigma_0^2)$

$p(\mu,\sigma^2) \propto \sigma^{-1}(\sigma^2)^{-(\upsilon_0/2+1)}exp \big(-\frac{1}{2\sigma^2}[\upsilon_0\sigma_0^2+\kappa_0(\mu_0-\mu^2)] \big)$

\bigskip

# joint likelihood

Derive explicitly the parameters of posterior distribution in terms of the prior parameters and the sufficient statistics of the data.

$p(y|\mu,\sigma^2) \propto (\sigma^2)^{-n/2}exp\big(-\frac{1}{2\sigma^2}(\sum(y_i-\mu)^2) \big)$

$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\propto (\sigma^2)^{-n/2}exp\big(-\frac{1}{2\sigma^2}(\sum(y_i-\bar{y})^2+n(\bar{y}-\mu)^2) \big)$

$(s^2,\bar{y})$ is sufficient for $(\mu,\sigma^2)$ where $s^2=\frac{1}{n-1}\sum(y_i-\bar{y})^2$

$p(y|\mu,\sigma^2) \propto (\sigma^2)^{-n/2}exp\big(-\frac{1}{2\sigma^2}(s^2(n-1)+n(\bar{y}-\mu)^2)\big)$

\bigskip

# joint posterior density 

$p(\mu,\sigma^2|y) \propto (\sigma^2)^{-n/2}exp\big(-\frac{1}{2\sigma^2}(s^2(n-1)+n(\bar{y}-\mu)^2)\big) \times \sigma^{-1}(\sigma^2)^{-(\upsilon_0/2+1)}exp \big(-\frac{1}{2\sigma^2}[\upsilon_0\sigma_0^2+\kappa_0(\mu_0-\mu^2)] \big)$

$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\propto \sigma^{-1}(\sigma^2)^{-((\upsilon_0+n)/2+1)}exp\big(-\frac{1}{2\sigma^2}[\upsilon_0\sigma_0^2+(n-1)s^2+\frac{n\kappa_0(\bar{y}-\mu_0)^2}{n+\kappa_0}+(n+\kappa_0)(\mu-\frac{\mu_0\kappa_0+n\bar{y}}{n+\kappa_0})^2] \big)$

$\mu,\sigma^2|y \sim N-Inv-\chi^2(\mu_n,\sigma_n^2/\kappa_n,\upsilon_n,\sigma_n^2)$

Where 

$\mu_n = \frac{\mu_0\kappa_0+n\bar{y}}{n+\kappa_0}= \frac{\kappa_0}{\kappa_0+n}\mu_0+\frac{n}{\kappa_0+n}\bar{y}$

$\kappa_n = \kappa_0 +n$

$\upsilon_n = \upsilon_0+n$

$\upsilon_n\sigma_n^2 = \upsilon_0\sigma_0^2+(n-1)s^2+\frac{\kappa_0n}{\kappa_0+n}(\bar{y}-\mu_0)^2$

\newpage

# Exercise 2 (3 points): 

Chapter 3, Exercise 10 from Bayesian Data Analysis, 3rd Edition (Gelman et al.). Note that for this exercise, you may use the result, without needing to prove it, that the quotient of two independent $\chi^2$ random variables, each divided by their respective degrees of freedom, has an F distribution.

## Chapter 3, Exercise 10

Comparison of normal variances: for j = 1, 2, suppose that

$$y_{j1},...,y_{jn_j}|\mu_j,\sigma_j^2\sim iid\;\; N(\mu_j,\sigma_j^2),$$
$$p(\mu_j,\sigma_j^2) \propto\;\;\sigma_j^{-2},$$

and $(\mu_1,\sigma_1^2)$ are independent of $(\mu_2,\sigma_2^2)$ in the prior distribution. Show that the posterior distribution of $(s_1^2/s_2^2)/(\sigma_1^2/\sigma_2^2)$ in $F$ with $(n_1-1)$ and $(n_2-1)$ degrees of freedom. (Hint: to show the required form of the posterior density, you do not need to carry along all the normalizing constants.)

\bigskip

$\sigma^2|y \sim Sclaed\; inverse\; \chi^2\; density(n-1,\sigma^2):$

$$P(\sigma^2|y) \propto (\sigma^2)^{-(n+1)/2}exp\big(-\frac{(n-1)s^2}{2\sigma^2}\big)$$

# change of variable

$P(\frac{(n-1)s^2}{\sigma^2}|y) = P(\sigma^2|y)\cdot\frac{(n-1)s^2}{\sigma^4} \propto (\sigma^2)^{-(n+1)/2}exp\big(-\frac{(n-1)s^2}{2\sigma^2}\big)\cdot \frac{(n-1)s^2}{\sigma^4}$

$P(\frac{(n-1)s^2}{\sigma^2}|y) \propto (\frac{(n-1)s^2}{\sigma^2})^{(n-1)/2-1}exp\big(-\frac{(n-1)s^2}{2\sigma^2}\big) \sim \chi^2_{n-1}$

\section{Properties of $\chi^2$ distribution}

if $T_1 \!\perp\!\!\!\perp T_2, T_1=\frac{v_1s_1^2}{\sigma_1^2}|y\sim \chi^2_{n_1-1}\;T_2=\frac{v_2s_2^2}{\sigma_2^2}|y\sim \chi^2_{n_2-1}$

then $\frac{T_1/v_1}{T_2/v_2}\sim F_{n_1-1,n_2-1}$

So

$(s_1^2/\sigma_1^2)/(s_2^2/\sigma_2^2)=(s_1^2/s_2^2)/(\sigma_1^2/\sigma_2^2)\sim F_{n_1-1,n_2-1}$ 

\newpage

# Exercise 3 (4 points): 

Suppose we wish to obtain samples from a Normal distribution with mean equal to zero and standard deviation equal to one, which is truncated to the range [-1.5, 1.5]. Further suppose that we cannot sample from such a distribution easily (e.g., using an R function), and so we use grid sampling to obtain a sample.

For this exercise, write code to implement the grid sampler described above; you do not need to include a uniform random jitter, as you will display the sample using a histogram that will bin them anyway. For the grid itself, use a grid width of 0.025. Obtain 5000 draws from the truncated Normal distribution using your grid sampler. Then make a histogram of the resulting sample, with the density of the distribution on the y-axis (i.e., you want to use **probability=TRUE** if youâ€™re making the histogram in R). Also plot the density curve corresponding to the truncated Normal distribution for comparison (in R, you may want to use the **dtruncnorm** function from the **truncnorm** package to do this).

Please turn in 

(i) the final histogram you produce, and 

(ii) the (commented) code you wrote to implement the grid sampler.

\bigskip

```{r}
#####Generate 1-d grid in range[-1.5,1.5]####
N = 5000
width <- 0.025
gridCenters <- seq(-1.5 + width/2, 1.5 - width/2, by = 0.025)

# Your codes here for posterior at centers and normalized terms

# probCells = ..  
probcells = (1/1)*exp(-1/2*((gridCenters-0)/1)^2)

# normalizedProbCells = ..
normalizedProbCells = probcells/sum(probcells)

#Sample according to grid probabilities
set.seed(460)
samples = sample(gridCenters, size = N, replace = TRUE, prob = normalizedProbCells)

# plot histogram of drawn samples and the true density curve
library(truncnorm)
hist(samples,
     probability = TRUE,
     breaks = seq(from = -1.5, to = 1.5, by = width),
     main = "Grid sampling N~(0,1)")
curve(dtruncnorm(x, -1.5, 1.5, 0, 1), from = -1.5, to = 1.5,
      col = 2, lwd = 2, add = TRUE)
legend("topleft", legend = "true density\n N~(0,1)", col = 2, lwd = 2)
```



```{r}
# solution
grid_samp <- function(grid_width,n_samps){
  # set up the grid
  my_grid <- seq(-1.5+grid_width/2,1.5-grid_width/2,grid_width)
  
  # obtain the density values at the grid points
  dens_vals <- dnorm(my_grid,mean=0,sd=1)
  
  # get the probability mass for each "cell"
  prob_mass <- dens_vals*grid_width
  
  # normalize by setting the total probability in the grid to 1
  prob_mass <- prob_mass/sum(prob_mass)
  
  # sample according to grid cell probabilities
  samp_grid <- sample(1:length(my_grid),n_samps,replace=TRUE,prob=prob_mass)
  samp_output <- my_grid[samp_grid]
  
  # output the samples
  return(samp_output)
}
```

```{r}
set.seed(101)
grid_width <- 0.025
samps <- grid_samp(grid_width=grid_width,n_samps=5000)
hist(samps,probability = TRUE,xlab="x",ylab="f(x)",main="")
xvals <- seq(-1.5,1.5,0.001)
lines(xvals,dtruncnorm(xvals,a=-1.5,b=1.5))
```

